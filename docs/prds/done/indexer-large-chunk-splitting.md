# Savant Indexer Large Chunk Splitting PRD

## Executive Summary
- **Objective:** Prevent oversized files from failing FTS insertion by splitting any chunk that exceeds Postgres limits into multiple sub-chunks during indexing.
- **Outcome:** Indexer handles arbitrarily large files without aborting the repo run, while search quality and scoring behavior stay consistent across existing repositories.

## Background
- **Current State:** `Savant::Indexer` chunkers emit a single chunk per logical slice (code by line spans, markdown by characters). For very large files or lenient chunk settings, a chunk may exceed Postgres' `text` storage and FTS tuple limit, causing insert failures that skip the entire file and leave repo stats inaccurate.
- **Pain:** Large autogenerated files (vendored schema dumps, JSON specs) are increasingly common; when they fail to insert we lose that file entirely, surfaces errors in logs, and stop trusting index completeness.
- **Why Now:** Users report intermittent failures on high-volume repos, blocking context search onboarding; splitting chunks is a bounded change that removes the blocker ahead of broader chunking roadmap work.

## Goals
- **Bounded Chunk Size:** Guarantee every row written to `chunks.chunk_text` stays below a configurable byte limit (default derived from 512 KB safety margin under Postgres TOAST thresholds).
- **Automatic Split:** Transparently split oversized chunks into sequential sub-chunks while preserving blob + chunk ordering and overlap semantics.
- **Observability:** Emit metrics / structured logs when splitting occurs so ops can monitor frequency and adjust config.
- **Resilience:** Ensure indexing of the rest of the repo continues even if a single sub-chunk still fails (surface actionable error but do not abort unless DB rejects all slices).

## Non-Goals
- **New Schema:** No additional tables or columns; reuse existing `chunks` schema.
- **User-Facing Config UI:** No CLI flags or MCP settings beyond config JSON fields for thresholds.
- **Search Algorithm Changes:** Ranking, highlighting, and scoring remain the same aside from smaller chunk granularity when splitting triggers.
- **Binary / Skip Logic:** No change to binary detection or ignore rules.

## Requirements & Acceptance Criteria
- **Threshold Config:** Add `indexer.chunk.maxChunkBytes` (or reuse `codeMaxLines`/`mdMaxChars` derivative) with sane default; missing value falls back to 512 KB.
- **Runtime Detection:** During `Chunkers::Base#chunk`, detect when the produced chunk string `bytesize > threshold`.
- **Split Strategy:**
  - For code chunks: re-split by line boundaries, maintaining overlap, until each piece fits.
  - For markdown/plaintext: split by character count to maintain readability.
  - Carry forward language tags and ordering; `idx` should increment per resulting piece.
- **Persistence:** `Savant::DB#replace_chunks` receives the expanded sequence w/out additional API changes.
- **Error Handling:** If Postgres still rejects an insert, log explicit error including repo, file, chunk index, and bytesize, then continue to next file.
- **Instrumentation:** Add counter/timer around split operations plus per-run summary (e.g., `split_chunks=count`, `max_chunk_bytes`, `files_split`).
- **Docs:** Update README + settings schema/example explaining new behavior and config flag.

## Architecture / Implementation Plan
1. **Config Extension:** Update `config/schema.json`, `settings.example.json`, and `Savant::Indexer::Config` to accept `maxChunkBytes`; expose via chunker settings.
2. **Chunker Utilities:** Add helper in `Chunkers::Base` (e.g., `split_if_needed(chunks, limit:, strategy:)`) reused by code/markdown chunkers.
3. **Indexer Runner:** Ensure chunk `idx` sequencing reflects expanded fragments (e.g., use enumerator that yields final flattened list before handoff to DB layer).
4. **Logger Hooks:** Use `Savant::Logger#info` + `with_timing` around splitting operations; include repo + file path metadata.
5. **Tests:** Add specs covering:
   - Config default + override behavior.
   - Code chunker splitting by lines and preserving overlap.
   - Markdown chunker splitting by char limit.
   - Runner handing flattened chunk list to DB and logging.

## Testing Strategy
- **Unit Specs:**
  - `spec/savant/indexer/chunker/code_chunker_spec.rb`: verify chunk sequences for synthetic files exceeding threshold.
  - `spec/savant/indexer/chunker/markdown_chunker_spec.rb`: confirm char-based splitting.
  - `spec/savant/indexer/config_spec.rb`: ensure new field validated and default applied.
- **Integration Spec:** Fake DB capturing chunk payload ensures `idx` monotonic and no chunk exceeds threshold.
- **Smoke Test:** Run `bin/context_repo_indexer index <repo>` against fixture repo containing oversized file; verify run completes and search returns fragments.

## Risks & Mitigations
- **Increased Chunk Count:** More rows per file may impact indexing time and query results. Mitigate via metrics and optional config to raise limit.
- **Overlap Semantics:** Incorrect splitting could drop context between sub-chunks. Mitigate with thorough tests ensuring overlap maintained for code chunker.
- **Config Rollout:** New config key must have backward compatible default; enforce optional field with fallback and highlight in docs.
- **DB Load:** More inserts per file may stress Postgres. Monitor repo indexing duration; consider batching if needed.

## Milestones
1. **Design & Config Update (Day 1):** Schema, docs, config loader changes reviewed.
2. **Chunker Implementation (Days 2-3):** Add splitting helpers, update chunkers, add unit specs.
3. **Runner Integration (Day 3):** Wire flattened chunks, logging, fake DB spec.
4. **Validation & Docs (Day 4):** Run smoke tests, update README + CHANGELOG, ship PR.

## Open Questions
- **Dynamic Threshold:** Should we expose per-language thresholds or allow runtime overrides via CLI flags?
- **Existing Data:** Do we need a backfill or migration to re-chunk files already stored with large single chunks, or is forward indexing sufficient?
- **Search Boosting:** Should sub-chunks created from splitting share the same score normalization, or should we tweak ranking to avoid favoring large files with many slices?
